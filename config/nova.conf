[DEFAULT]
osapi_compute_workers = {{ openstack_mandatory_service_workers }}
metadata_workers  = {{ openstack_mandatory_service_workers }}
resize_confirm_window = 2
resume_guests_state_on_host_boot = true
reclaim_instance_interval = {{ nova_reclaim_instance_interval }}
instance_usage_audit = false
{% raw %}
reserved_host_memory_mb = {{ nova_reserved_host_memory_mb }}
{% endraw %}
#vcpu_pin_set =

[conductor]
workers = {{ openstack_mandatory_service_workers }}

[filter_scheduler]
enabled_filters = ComputeFilter,AvailabilityZoneFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter,DifferentHostFilter,SameHostFilter,AggregateInstanceExtraSpecsFilter,AggregateImagePropertiesIsolation,AggregateMultiTenancyIsolation,AggregateTypeAffinityFilter,NUMATopologyFilter,PciPassthroughFilter
weight_classes = nova.scheduler.weights.ram.RAMWeigher

[neutron]
#default_floating_pool=? (nova? public? BGP?)

# List of physnets present on this host.
#
# For each *physnet* listed, an additional section,
# ``[neutron_physnet_$PHYSNET]``, will be added to the configuration file. Each
# section must be configured with a single configuration option, ``numa_nodes``,
# which should be a list of node IDs for all NUMA nodes this physnet is
# associated with. For example::
#
#     [neutron]
#     physnets = foo, bar
#
#     [neutron_physnet_foo]
#     numa_nodes = 0
#
#     [neutron_physnet_bar]
#     numa_nodes = 0,1
#
# Any *physnet* that is not listed using this option will be treated as having
# no
# particular NUMA node affinity.
#
# Tunnelled networks (VXLAN, GRE, ...) cannot be accounted for in this way and
# are instead configured using the ``[neutron_tunnel]`` group. For example::
#
#     [neutron_tunnel]
#     numa_nodes = 1
#
# Related options:
#
# * ``[neutron_tunnel] numa_nodes`` can be used to configure NUMA affinity for
#   all tunneled networks
# * ``[neutron_physnet_$PHYSNET] numa_nodes`` must be configured for each value
#   of ``$PHYSNET`` specified by this option
#  (list value)
#physnets =

[compute]
# Defines which physical CPUs (pCPUs) will be used for best-effort guest vCPU
# resources.
#
# Currently only used by libvirt driver to place guest emulator threads when
# hw:emulator_threads_policy:share.
#
# ::
#     cpu_shared_set = "4-12,^8,15"
#  (string value)
#https://specs.openstack.org/openstack/nova-specs/specs/ocata/approved/libvirt-emulator-threads-policy.html
#cpu_shared_set? (qemu threads for running system process,not vcpu, used in dedicated cpu scenario)

[libvirt]
disk_cachemodes = network=none
rx_queue_size=512
tx_queue_size=512

#volume_use_multipath =

{% if enable_barbican | bool %}
[barbican]
region_name = {{ openstack_region_name }}
barbican_endpoint_type = internal
verify_ssl_path = {{ openstack_cacert }}
{% endif %}

[scheduler]
workers = {{ openstack_mandatory_service_workers }}
